{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepQ\n",
    "- Sara Echeverría 21371\n",
    "- Ricardo Mendez 21289\n",
    "- Melissa Pérez 21385\n",
    "\n",
    "Repository link: https://github.com/bl33h/deepQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import torch.optim as optim\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dql neural network model\n",
    "class DQL(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQL, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # first hidden layer\n",
    "        self.fc2 = nn.Linear(128, 128)        # second hidden layer\n",
    "        self.fc3 = nn.Linear(128, output_dim) # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # relu activation on the first layer\n",
    "        x = torch.relu(self.fc2(x))  # relu activation on the second layer\n",
    "        return self.fc3(x)           # output layer, no activation due to q-value output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "gamma = 0.99  # discount factor\n",
    "epsilon = 1.0  # initial exploration rate\n",
    "epsilonDecay = 0.995  # epsilon decay per episode\n",
    "epsilonMin = 0.01  # minimum epsilon\n",
    "learningRate = 0.001  # learning rate\n",
    "batchSize = 64  # batch size for experience replay\n",
    "memorySize = 10000  # memory capacity\n",
    "targetUpdateFreq = 100  # target network update frequency\n",
    "numEpisodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the network and optimizer\n",
    "inputDim = env.observation_space.shape[0]\n",
    "outputDim = env.action_space.n\n",
    "policyNet = DQL(inputDim, outputDim)\n",
    "targetNet = DQL(inputDim, outputDim)\n",
    "targetNet.load_state_dict(policyNet.state_dict())\n",
    "targetNet.eval()  # set target network to evaluation mode\n",
    "optimizer = optim.Adam(policyNet.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory for experience replay\n",
    "memory = deque(maxlen=memorySize)\n",
    "\n",
    "# function to choose an action based on epsilon-greedy approach\n",
    "def selectAction(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # explore: select a random action\n",
    "    else:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return policyNet(state).max(1)[1].view(1, 1).item()  # exploit: select the best action based on the current policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "for episode in range(numEpisodes):\n",
    "    state = env.reset()\n",
    "    totalReward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = selectAction(state, epsilon)\n",
    "        # unpack with the additional boolean\n",
    "        nextState, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        memory.append((state, action, reward, nextState, done))\n",
    "        state = nextState\n",
    "        totalReward += reward\n",
    "\n",
    "    print(f\"Episode {episode}: Total reward = {totalReward}\")\n",
    "\n",
    "# cleanup\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
